{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP 3 Aprendizaje de Máquinas\n",
    "---\n",
    "## Árboles de Decisión y Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1: Análisis de dataset Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para demostrar el uso de árboles de decisión y random forest vamos a usar el dataset Iris. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset Iris tiene cuatro características (`sepal length`, `sepal width`, `petal length`, `petal width`) que se pueden usar para clasificar las flores de Iris en tres especies indicadas como \"0\", \"1\", \"2\" (setosa, versicolor, virginica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['species'] = iris.target\n",
    "df['species_names'] = df.species\n",
    "df.replace({'species_names':{\n",
    "            0:iris['target_names'][0],\n",
    "            1:iris['target_names'][1],\n",
    "            2:iris['target_names'][2]            \n",
    "        }}, inplace=True)\n",
    "df.columns = [item.replace(' (cm)', '') for item in df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el formato de este dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Busquemos la cantidad de elementos que pertenecen a cada una de las clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.species.value_counts()\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# np.unique(df.species, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedamos a dividir el dataset en un conjunto de entrenamiento y otro de prueba. Se quiere entrenar usando el 70% del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2: Decision Tree aplicado a Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar el funcionamiento de los árboles de decisión, usémoslo en el problema de clasificar las especies de Iris. La implementación por defecto de `sklearn` para árboles de Decisión, `DecisionTreeClassifier` tiene como medida de calidad de la separación de los nodos `gini`, para **Impureza Gini**, para usar la medida dada en clases, **Ganancia de Información**, tenemos que poner el parámetro opcional `criterion` igual a `entropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "tree_clf.fit(X_train, y_train)\n",
    "tree_clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las reglas del árbol de decisión pueden ser representadas usando un grafo, mediante la función de `sklearn` `plot_tree(decision_tree)` que recibe además como parámetros opcionales, `feature_names` para representar los nombres de las características y `class_names`, los nombres de las clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = plot_tree(tree_clf, \n",
    "                   feature_names=iris.feature_names,  \n",
    "                   class_names=iris.target_names,\n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulta que solo se necesitan dos parámetros, la longitud del pétalo (`petal length`) y el ancho del pétalo (`petal width`), para clasificar la mayoría de los muestras. El parámetro `sepal width`, para el ancho del sépalo, también se usa para hacer las distinciones más finas, pero en última instancia no aporta mucho valor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3: Random Forest aplicado a Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos ahora los resultados que se obtienen con Random Forest en el dataset Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf = RandomForestClassifier(criterion=\"entropy\")\n",
    "forest_clf.fit(X_train, y_train)\n",
    "forest_clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 4: Visualizando la Importancia de las Características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una gran ventaja de los clasificadores basados en árboles es que nos permite hacernos una idea de la importancia relativa de cada carcterística en función de como se dividen los nodos en la fase de entrenamiento. Para ello, tanto el `DecisionTreeClassifier` como el `RandomForestClassifier` de `scikit-learn` proporciona un atributo llamado `feature_importances_`. Esto devuelve un arrray de valores que suman 1. Cuanto mayor sea la puntuación, más importante será la característica. La puntuación se calcula como la reducción total (normalizada) del criterio aportado por esa característica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_feature_importance(feature_imp: pd.Series):\n",
    "    \"Grafica la importancia de cada característica\"\n",
    "    sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title(\"Visualizing Important Features\", pad=15, size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, se grafica la importancia de las características según los resultados obtenidos por el árbol de decisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.Series(tree_clf.feature_importances_, \n",
    "                        index=['sepal length', 'sepal width', 'petal length', 'petal width']).sort_values(ascending=False)\n",
    "plot_feature_importance(feature_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y luego se grafica la importancia de las características del Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.Series(forest_clf.feature_importances_, \n",
    "                        index=['sepal length', 'sepal width', 'petal length', 'petal width']).sort_values(ascending=False)\n",
    "plot_feature_importance(feature_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 5: Visualizar los espacios de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado del árbol de decisión se puede visualizar graficando su espacio de decisión. A continuación se implementa una función (`mesh_plot`) que muestra este resultado utilizando regiones sombreadas que coinciden con los colores utilizados para identificar la flor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'figure.titlesize': 'large'})\n",
    "step = 0.04\n",
    "\n",
    "def mesh_plot(x: pd.DataFrame, y: pd.Series, species: pd.Series, ax: plt.Axes, clf):\n",
    "    values = species.unique()\n",
    "    colors = sns.color_palette()[:len(values)]\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x.min() - 0.1, x.max() + 0.1, step),\n",
    "        np.arange(y.min() - 0.1, y.max() + 0.1, step))\n",
    "    mesh_predict = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    mesh_predict = mesh_predict.reshape(xx.shape)\n",
    "    for i in range(len(colors)):\n",
    "        ax.scatter(x[df.species == values[i]], y[df.species == values[i]], color=colors[i])\n",
    "        ax.set_xlim(x.min() - 0.2, x.max() + 0.2)\n",
    "        ax.set_ylim(y.min() - 0.2, y.max() + 0.2)\n",
    "    ax.pcolormesh(xx, yy, mesh_predict,\n",
    "        cmap=ListedColormap(sns.color_palette()[:3]), alpha=0.2, shading='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `plot_features`, dado un dataset representado por `df`, un par de features `feat1` y `feat2`, un tipo de clasificador de árbol especificado por `clsf` y un eje `ax`, grafica el espacio de decisión de dicho clasificador en el eje `ax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features(df: pd.DataFrame, feat1: str, feat2: str, clsf, ax: plt.Axes):\n",
    "    \"\"\"\n",
    "    Dado un dataset representado por `df`, un par de features `feat1` y `feat2`, un tipo de clasificador de árbol \n",
    "    especificado por `clsf` y un eje `ax`, grafica el espacio de decisión de dicho clasificador en el eje `ax`.\n",
    "    \"\"\"\n",
    "    X = df[[feat1, feat2]]\n",
    "    y = df.species\n",
    "    fit_clsf = clsf().fit(X, y)\n",
    "    ax.set(xlabel=feat1, ylabel=feat2)\n",
    "    mesh_plot(df[feat1], df[feat2], df.species, ax, fit_clsf) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora grafiquemos el espacio de decisión de 4 pares de características del clasificador `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(8, 8))\n",
    "# fig.tight_layout()\n",
    "fig.suptitle('Decision Tree Decision Space')\n",
    "\n",
    "plot_features(df, 'petal length', 'petal width', DecisionTreeClassifier, ax1)\n",
    "plot_features(df, 'sepal length', 'sepal width', DecisionTreeClassifier, ax2)\n",
    "plot_features(df, 'sepal length', 'petal length', DecisionTreeClassifier, ax3)\n",
    "plot_features(df, 'sepal width', 'petal width', DecisionTreeClassifier, ax4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, podemos hacer los mismo con `RandomForestClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(8, 8))\n",
    "# fig.tight_layout()\n",
    "fig.suptitle('Random Forest Decision Space')\n",
    "\n",
    "plot_features(df, 'petal length', 'petal width', RandomForestClassifier, ax1)\n",
    "plot_features(df, 'sepal length', 'sepal width', RandomForestClassifier, ax2)\n",
    "plot_features(df, 'sepal length', 'petal length', RandomForestClassifier, ax3)\n",
    "plot_features(df, 'sepal width', 'petal width', RandomForestClassifier, ax4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede notar como los espacios de decisión de los árboles de decisión son más rectos, mientras que el de _Random Forest_ tiene curvas un poco más suaves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 6: Profundidad de los árboles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando un árbol de decisión con poca profundidad los resultados obtenidos en el dataset no son buenos. A medida que aumenta la profundidad, el árbol de decisiones identifica mejor las especies de Iris. Esto lo podemos comprobar graficando el espacio de decisión para observar los ejemplos que son clasificados mal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def plot_dt_by_depth(df: pd.DataFrame, feat1: str, feat2: str, ax: List[plt.Axes]):\n",
    "  \"\"\"\n",
    "  Grafica el espacio de decisión de un árbol de decisión según `feat1` y `feat2`\n",
    "  usando los datos presentes en un dataframe `df` en los ejes `ax`, que está compuesto\n",
    "  por 3 ejes.\n",
    "  \"\"\"\n",
    "  # La matriz de características solo está compuesta por las características de interés\n",
    "  X = df[[feat1, feat2]]\n",
    "  for idx in range(0, 3):\n",
    "    # Se crea el árbol de decisión de clasificación con la profundidad determinada, y se realiza el entrenamiento \n",
    "    clf = DecisionTreeClassifier(max_depth=idx + 1, random_state=0).fit(X, df.species)\n",
    "    # Se grafica el espacio de decisión\n",
    "    mesh_plot(df[feat1], df[feat2], df.species, ax[idx], clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, podemos graficar el árbol teniendo en cuenta dos características: `petal length` y `petal width`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crean los subplots\n",
    "fig, ax = plt.subplots(1, 3, sharey=True, figsize=(15, 5), squeeze=True)\n",
    "fig.tight_layout()\n",
    "fig.suptitle('Decision trees with varying depths', y=1.05)\n",
    "\n",
    "plot_dt_by_depth(df, 'petal length', 'petal width', ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 7: Decision Tree y Random Forest aplicados al dataset de _Rotten Tomatoes_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, probemos los nuevos clasificadores en la tarea de aprendizaje anterior, la clasificación de críticas de _Rotten Tomatoes_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como en la clase práctica anterior, extraemos el contenidos de los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path_p = Path(\"txt_sentoken/pos\")\n",
    "path_n = Path(\"txt_sentoken/neg\")\n",
    "\n",
    "ds_p = list(path_p.iterdir())     # directorio donde están las críticas positivas\n",
    "ds_n = list(path_n.iterdir())     # directorio donde están las críticas negativas\n",
    "\n",
    "def convert_file_to_text(file_path: Path) -> str:\n",
    "    with open(file_path) as f:\n",
    "        return ''.join(f.readlines())\n",
    "    \n",
    "texts_p = [convert_file_to_text(file) for file in ds_p]    # Lista de críticas positivas\n",
    "texts_n = [convert_file_to_text(file) for file in ds_n]    # Lista de críticas negativas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y creamos la matriz de características y el vector de clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "mt = vectorizer.fit_transform(texts_p + texts_n)\n",
    "mta = mt.toarray()\n",
    "\n",
    "y = [1]*1000 + [0]*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante esta función realizamos los experimentos para comprobar el rendimiento promedio de los algoritmos en varias iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiments(Clsf, iterations: int) -> List[float]:\n",
    "    rs = []\n",
    "    for _ in range(iterations):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(mta, y, train_size=0.60)\n",
    "        clf = Clsf(criterion=\"entropy\")\n",
    "        clf.fit(X_train, y_train)\n",
    "        rs.append(clf.score(X_test, y_test))\n",
    "    return rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora probemos los resultados con `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dt = experiments(DecisionTreeClassifier,30)\n",
    "np.mean(results_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y con `RadomForestClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rf = experiments(RandomForestClassifier,30)\n",
    "np.mean(results_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
