{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbe0c4f3-5721-486a-a033-f67f21e8b663",
   "metadata": {},
   "source": [
    "# CP7  Aprendizaje de Máquinas\n",
    "---\n",
    "## Algoritmos de Reducción de Dimensiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9ca098-e42d-4923-b924-d3ce1b3b6ba9",
   "metadata": {},
   "source": [
    "Los datos de muchas dimensiones presentan un reto para los modelos estadísticos. Afortunadamente, gran parte de los datos son redundantes y pueden reducirse a un número menor de variables sin perder mucha información.\n",
    "\n",
    "Normalmente, usamos la reducción de dimensionalidad en el aprendizaje automático y la exploración de datos. En el aprendizaje automático, lo usamos para reducir la cantidad de características. Esto disminuirá la potencia computacional y posiblemente conducirá a un mejor rendimiento del modelo.\n",
    "\n",
    "De manera similar, podemos usar la reducción de dimensionalidad para proyectar datos en dos dimensiones. Dicha visualización puede ayudarnos a detectar valores atípicos o clusters de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7749549a-eacf-4b21-a436-c728c4fbfe9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "En esta clase, compararemos cuatro métodos diferentes para lograr dicha tarea: (1) Principle Component Analysis (PCA), (2) Kernel Principle Component Analysis (kPCA) y (3) Linear Discriminant Analysis (LDA) y (4) t-distributed Stochastic Neighbouring Entities (t-SNE). Para esto, usaremos el conjunto de datos de Iris provisto dentro de `scikit-learn`, el cual consta de 150 muestras, cada una con 4 características."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add12b11-bc8b-4543-aac9-c8ea7a6f149d",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Análisis del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507a4f5d-dc78-4399-adbb-488adfdbe574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542f54fd-97a8-4389-912a-d309474c0440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris_dataset = datasets.load_iris()\n",
    "X = iris_dataset.data \n",
    "y = iris_dataset.target\n",
    "target_names = iris_dataset.target_names\n",
    "\n",
    "iris_df = pd.DataFrame(iris_dataset.data, columns = iris_dataset.feature_names)\n",
    "iris_df['Species']=iris_dataset['target']\n",
    "iris_df['Species']=iris_df['Species'].apply(lambda x: iris_dataset['target_names'][x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce645ee2-e2db-4dbd-b7b4-36de221370e4",
   "metadata": {},
   "source": [
    "Analicemos el contenido del dataset mediante una muestra de sus primeros elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ce6696-241c-4fa3-bf8e-a467f5e19d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver el contenido del dataset\n",
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce656a58-7bd9-4c94-997a-8c79d3d01f1f",
   "metadata": {},
   "source": [
    "Veamos que contiene 3 clases, donde cada clase se refiere a un tipo de planta Iris y está representada en 50 elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be7ba7b-1a03-40f6-b1c4-d8146c433df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver la cantidad de clases\n",
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b93584-3fb0-44fd-b146-1702f4d88fbe",
   "metadata": {},
   "source": [
    "También podemos ver cómo se separan las clases en función de las diferentes características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1898251-6c49-4e7c-a0a8-8bf1f7e4d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {'Setosa':'#FCEE0C','Versicolor':'#FC8E72','Virginica':'#FC3DC9'}\n",
    "\n",
    "#Let see how the classes are separated regarding different featueres\n",
    "\n",
    "sns.FacetGrid(iris_df, hue=\"Species\", height=4, palette=colors.values()) \\\n",
    "   .map(plt.scatter, \"sepal length (cm)\", \"sepal width (cm)\") \\\n",
    "   .add_legend()\n",
    "\n",
    "\n",
    "sns.FacetGrid(iris_df, hue= \"Species\", height=4, palette=colors.values()).\\\n",
    "map(plt.scatter, \"petal length (cm)\", \"petal width (cm)\").add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0375cac-6d0a-4962-bd3b-1941f1b0cf35",
   "metadata": {},
   "source": [
    "La matriz de correlación puede ayudarnos a comprender mejor el conjunto de datos. Nos dice cómo se correlacionan nuestras cuatro características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc530080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la matriz de correlación\n",
    "corr = None     # TODO: Your code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c55b2a-50e4-441d-917a-5d80bc7658ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(corr, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b9b854-3f60-4b90-ae31-a791a091d1c9",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c7c322-5990-4a02-9450-1daae8e2fe62",
   "metadata": {},
   "source": [
    "En pocas palabras, PCA es un algoritmo de transformación lineal que busca proyectar las características originales de nuestros datos en un conjunto más pequeño de características (o subespacio) mientras conserva la mayor parte de la información. Para hacer esto, el algoritmo intenta encontrar las direcciones/ángulos más apropiados (que son los componentes principales) que maximizan la varianza en el nuevo subespacio. Sin embargo, ¿por qué maximizar la varianza?\n",
    "\n",
    "Para responder a la pregunta, se debe dar más contexto sobre el método PCA. Uno tiene que entender que los componentes principales son ortogonales entre sí (piense en ángulo recto). Como tal, al generar la matriz de covarianza (medida de cuán relacionadas están 2 variables entre sí) en nuestro nuevo subespacio, los valores fuera de la diagonal de la matriz de covarianza serán cero y solo las diagonales (o valores propios) serán distintos de cero. Son estos valores diagonales los que representan las *varianzas* de los componentes principales de los que estamos hablando o información sobre la variabilidad de nuestras características.\n",
    "\n",
    "Por lo tanto, cuando PCA busca maximizar esta varianza, el método está tratando de encontrar direcciones (componentes principales) que contengan la mayor dispersión/subconjunto de puntos de datos o información (varianza) relativa a todos los puntos de datos presentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ae336-4689-4698-ac1c-3b85a97997ab",
   "metadata": {},
   "source": [
    "### ¿Cuándo se debería usar PCA?\n",
    "\n",
    "Es importante tener en cuenta que PCA funciona bien con variables altamente correlacionadas. Si la relación entre las variables es débil, PCA no será efectivo. Puede mirar la matriz de correlación para determinar si usar PCA. Si la mayoría de los coeficientes son inferiores a 0,3, no es una buena idea utilizar PCA.\n",
    "\n",
    "Además, se pueden mirar los coeficientes de correlación para determinar qué variables están altamente correlacionadas. Si encuentra tales variables, puede usar solo una de ellas en el análisis. Un límite para altamente correlacionado suele ser 0,8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6275c-0831-4a45-a6d5-aa11467f4b9d",
   "metadata": {},
   "source": [
    "## Ejercicio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c832e-3b32-47db-898c-9543cd704810",
   "metadata": {},
   "source": [
    "Ahora, apliquemos PCA con 2 componentes. Esto nos ayudará a representar nuestros datos en dos dimensiones.\n",
    "\n",
    "Primero, necesitamos normalizar las características. Para esto usaremos la clase `StandardScaler` mediante su método `fit_transform` para procesar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e679bf7-4369-42a4-b1b4-25801048f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d604f818-4794-436d-add2-9cb63a41a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use standard scaler to normalize the features\n",
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43aef88-3318-44c6-b048-f54186c89bce",
   "metadata": {},
   "source": [
    "Después de la normalización, podemos transformar nuestras características usando `PCA` con dos componentes mediante el metodo `fit_transform`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b403229-4bc8-4d78-aa90-0e71f7f1e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cdd1bc-847e-4355-8699-49ecb6ce889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to reduce the features\n",
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260cb3b1",
   "metadata": {},
   "source": [
    "Grafiquemos ahora nuestro dataset con las dimensiones reducidas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa7102e-96a2-4f26-bd9a-a585d3c68d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reduced_data(dr_method, X_r, title: str='Reduced Data'):\n",
    "      'Plot the reduced data using The specified dimension reduction method'\n",
    "      plt.figure(figsize=(8,6))\n",
    "      \n",
    "      for color, i, target_name in zip(colors.values(), [0, 1, 2], target_names):\n",
    "            plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, \n",
    "                        label=target_name, s=130, edgecolors='k')\n",
    "      plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "      plt.xlabel('1st Dimension')\n",
    "      plt.ylabel('2nd Dimension')\n",
    "      plt.title(title)\n",
    "\n",
    "      # Percentage of variance explained for each components\n",
    "\n",
    "      plt.show()\n",
    "      \n",
    "      try:\n",
    "            print('explained variance ratio (first two components): %s' # First two PCA components capture 0.9776852*100% of total variation!\n",
    "                        %str(dr_method.explained_variance_ratio_))\n",
    "      except AttributeError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29b6998",
   "metadata": {},
   "source": [
    "Grafiquemos nuestro dataset con las dimensiones reducidas usando la función anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f7620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e833ae2c-0b46-406b-a3c6-0e46ccc880df",
   "metadata": {},
   "source": [
    "Podemos ver que Iris Setosa es muy diferente de las otras dos clases. Además, podemos calcular la varianza explicada. La varianza explicada nos dirá cuánto de la varianza ocupan nuestros dos componentes.\n",
    "\n",
    "Obtuvimos un resultado de 95.8%, en total para los dos primeros componentes. Esto significa que los dos primeros componentes principales ocupan el 95,8% de la varianza. Este es un buen resultado y significa que nuestra representación 2D es significativa. Si esta puntuación fuera inferior al 85 %, significaría que nuestra representación 2D de los datos podría no ser válida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e73c21-88ec-48cd-8e3e-3d2a7cb2abad",
   "metadata": {},
   "source": [
    "## Kernel PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b97dcb-31d3-467e-8e7e-0282838e6a31",
   "metadata": {},
   "source": [
    "Kernel PCA es una de las variaciones de PCA en la que usamos métodos kernel para realizar análisis de componentes principales con conjuntos de datos no linealmente separables. Este enfoque es muy similar al estándar pero con un paso de procesamiento diferente.\n",
    "\n",
    "Un conjunto de datos no lineal de baja dimensión a menudo puede volverse linealmente separable si se proyecta en un espacio especial de alta dimensión. El enfoque Kernel hace lo mismo, logra este objetivo sin tener que usar operaciones no lineales muy difíciles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc1ecc6-16d6-4f6b-8dc9-c29b369cfb0e",
   "metadata": {},
   "source": [
    "### ¿Cuándo debería usar Kernel PCA?\n",
    "\n",
    "El método Kernel de análisis de componentes principales es un enfoque poderoso cuando el conjunto de datos está formado por elementos que pueden ser una función de los componentes principales, pero no podemos determinar una relación lineal entre ellos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79067a6f-c114-4fd7-b89b-0dc3df3684b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ejercicio 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdfe294-750b-49d4-be01-b2cb66638ac9",
   "metadata": {},
   "source": [
    "A continuación apliquemos Kernel PCA a nuestro dataset. Primero probaremos usando un kernel polinomial. La configuración que se sugiere es  `n_components=2, kernel='poly', degree=2, gamma=1, coef0=0.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1466d6ba-ac56-4c3c-8be1-252a2425183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9c2ce0-6250-42d5-b873-3c4f989b9145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create polynomial Kernel and apply it to the data\n",
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46167bb",
   "metadata": {},
   "source": [
    "Grafiquemos los datos en dimensiones reducidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b5434a-876a-423f-af78-a97f5c2f7004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e264354a-b4ce-496b-b210-26bb77c2bee0",
   "metadata": {},
   "source": [
    "Ahora vamos a aplicar un kernel radial (`kernel='rbf'`) también con 2 componentes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f02f79-f45a-4e14-be82-bc09dbb97093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF Kernel\n",
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66565eb4",
   "metadata": {},
   "source": [
    "Y grafiquemos los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901a1f55-6d7c-4c17-ba29-2fbb19c657cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc8d45d-3e95-48b3-9aa1-b828ddabfc43",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf0bd42-8989-44fd-9eec-6afd2d5c21d9",
   "metadata": {},
   "source": [
    "A diferencia de PCA, que es un algoritmo de aprendizaje no supervisado, LDA pertenece a la clase de métodos de aprendizaje supervisado. Como tal, el objetivo de LDA es que con la información disponible sobre las etiquetas de clase, LDA buscará maximizar la separación entre las diferentes clases calculando los ejes de los componentes (discriminantes lineales)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49e586c-0ad6-4065-89ff-24b1ed7bddb3",
   "metadata": {},
   "source": [
    "### ¿Cuándo debería usar LDA?\n",
    "\n",
    "Podemos usar LDA solo para aprendizaje supervisado. Esto significa que necesitamos conocer las etiquetas de clase de antemano.\n",
    "\n",
    "Algunos experimentos compararon la clasificación al usar PCA o LDA. Estos experimentos muestran que la precisión de la clasificación tiende a mejorar cuando se usa PCA. Finalmente, el rendimiento de estas técnicas depende en gran medida de las características del conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab1504e-5882-46a5-bb33-e5f6560e0e22",
   "metadata": {},
   "source": [
    "## Ejercicio 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f972fd2-4aa6-4af8-bdc0-d5449a5a4a5b",
   "metadata": {},
   "source": [
    "Calculemos los dos primeros componentes LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134aecff-9bd7-495e-b3c8-bdc385db4b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15badc2",
   "metadata": {},
   "source": [
    "Apliquemos LDA en nuestro dataset. \n",
    "\n",
    "Nota: LDA es supervisado, por lo tanto, el método `fit` requiere la información de las etiquetas de clase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd040cda-292f-44b1-ac27-c9a9f20198ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09e58dd-08ac-4f91-986a-ffa8eb2d1c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos los datos reducidos\n",
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9206fd-7362-4e00-803c-1e1f7a3806f3",
   "metadata": {},
   "source": [
    "Tanto en PCA como en LDA, los datos de Setosa están bien separados de las otras dos clases. Además, podemos ver que LDA funciona mejor al mantener al mínimo la superposición entre Versicolor y Virginica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e5ee8-80bd-4cd3-b1d1-bfe778809e10",
   "metadata": {},
   "source": [
    "Cada uno de los tres métodos hasta ahora produjo esencialmente una nueva característica al considerar la transformación lineal/no lineal de las características originales. Al observar solo el primer componente principal de cada uno de los tres métodos hasta ahora, ¿qué método proporcionó la característica más útil si uno estuviera interesado en una tarea de clasificación?\n",
    "\n",
    "LDA brindaría información más útil en este caso, ya que su objetivo es separar los datos de diferentes clases. Por otro lado, PCA o kernel PCA es en realidad independiente de la clasificación de cada punto. El objetivo principal de PCA o kernel PCA es maximizar la varianza. Sin embargo, no es cierto a priori que las características con varianza máxima tengan la mayor (o incluso ninguna) importancia en términos de clasificación de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba748c-3602-45a3-9827-5c83bca144b3",
   "metadata": {},
   "source": [
    "## t-distributed Stochastic Neighbouring Entities (t-SNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2487e0a4-efbf-4f51-811f-d8bc64b1f061",
   "metadata": {},
   "source": [
    "t-SNE es una valiosa técnica de visualización de datos. Es no supervisado y no lineal. t-SNE tiene una función de costo que es no convexa. Por lo tanto, diferentes inicializaciones pueden conducir a diferentes mínimos locales. Si el número de características es muy alto, se recomienda utilizar primero otra técnica para reducir el número de dimensiones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae4730-5fdd-4906-9ac0-e583792a7e13",
   "metadata": {},
   "source": [
    "Aunque t-SNE podría no ser adecuado para el preprocesamiento de datos, sigue siendo útil para visualizar los datos y también para proporcionar información útil (aunque a veces engañosa) sobre las propiedades de agrupación de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb2291c-3582-4072-98ad-db3455585443",
   "metadata": {},
   "source": [
    "### ¿Cuándo debería usar t-SNE?\n",
    "\n",
    "t-SNE coloca a los vecinos cerca unos de otros, por lo que no podemos ver claramente cómo se relacionan las muestras con respecto a sus características. Se utiliza para la exploración de datos, especialmente para visualizar datos de alta dimensión.\n",
    "\n",
    "t-SNE no aprende una función del espacio original al nuevo. Debido a esto, no puede mapear los nuevos datos de acuerdo con los resultados anteriores de t-SNE. En otras palabras, no se puede utilizar en modelos de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf52e8b-1c3f-4ceb-902c-24e1b8fdb0c7",
   "metadata": {},
   "source": [
    "## Ejercicio 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1621ce4-9d6f-49da-a25b-47563c494c2b",
   "metadata": {},
   "source": [
    "Visualicemos nuestro conjunto de datos usando t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c342d7d-728b-4068-a993-7c2cab5e1e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf968dc",
   "metadata": {},
   "source": [
    "Apliquemos t-SNE en nuestro dataset.\n",
    "\n",
    "Nota: t-SNE no está pensado para ser usada en datos nuevos, así que no tiene el método `transform`. Para obtener los datos en dimensiones reducidas, se debe usar el método `fit_transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8249e7-76a6-4948-b56f-da7c493e3fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613da936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafiquemos los datos en dimensiones reducidas\n",
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58ed64d-ee4c-49e2-848e-63779762b0a9",
   "metadata": {},
   "source": [
    "Como se aprecia obtenemos una mejora significativa con respecto a PCA y LDA ya que las especies de Iris forman grupos definidos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0739b1-84f5-4cc9-b390-db8887dfad25",
   "metadata": {},
   "source": [
    "## Ejercicio 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fa9f41",
   "metadata": {},
   "source": [
    "\n",
    "Vamos a aplicar estos algoritmos de reducción de dimensiones como paso de preprocesamiento para la solución de un problema de aprendizaje. Para ello vamos a comparar diferentes métodos de reducción de dimensionalidad (lineal) aplicados en un conjunto de datos de dígitos (`Digits`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8c75f9",
   "metadata": {},
   "source": [
    "A continuación, se definen variables para la configuración de los algoritmos utilizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291bedfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 3\n",
    "random_state = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79887912",
   "metadata": {},
   "source": [
    "Luego, cargamos los datos. El conjunto de datos contiene imágenes de dígitos del 0 al 9 con aproximadamente 180 muestras de cada clase. Cada imagen tiene una dimensión de 8x8 = 64 y se reduce a un punto de datos bidimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c3e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Digits dataset\n",
    "X, y = datasets.load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a366702",
   "metadata": {},
   "source": [
    "Dividimmos nuestro conjunto de datos en dos conjuntos: uno para entrenamiento y otro para prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6018cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e833f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, stratify=y, random_state=random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7066f2b",
   "metadata": {},
   "source": [
    "Veamos las dimensiones de nuestro conjunto de características y la cantidad de clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48766771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f03dfc2",
   "metadata": {},
   "source": [
    "Para utilizar reducción de dimensiones, vamos a utilizar un `Pipeline` de `sklearn`.\n",
    "\n",
    "Un `Pipeline` aplica secuencialmente una lista de transformaciones y un estimador final. Los pasos intermedios del pipeline deben ser 'transformaciones', es decir, deben implementar métodos de `fit` y `transform`. El estimador final solo necesita implementar el método `fit`.\n",
    "\n",
    "Para contruir un pipeline, podemos usar la función `make_pipeline`, que recibe una lista de transformadores. En nuestro caso, lo usaremos para normalizar los datos (usando `StandardScaler`) y un método de reducción de dimensiones. Nos interesan PCA y LDA.\n",
    "\n",
    "Recordar que antes de aplicar una transformación, los datos deben estar normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33abe9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c844a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimension to 2 with PCA\n",
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ed9d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimension to 2 with LinearDiscriminantAnalysis\n",
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77afeb31",
   "metadata": {},
   "source": [
    "Luego, usaremos el clasificador de KNN para evaluar los métodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f25555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2044ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=n_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc18dfc4",
   "metadata": {},
   "source": [
    "Y creamos una lista de los métodos a comparar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc9c621",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_reduction_methods = []  # TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b933fc81",
   "metadata": {},
   "source": [
    "Luego resolvemos el problema de clasificación usando los métodos de reducción de dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2b8b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, model) in enumerate(dim_reduction_methods):\n",
    "    plt.figure()\n",
    "\n",
    "    # Ajustamos el modelo del pipeline (es decir, el método de reducción de dimensiones)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Ajuste un clasificador de knn en el conjunto de entrenamiento transformado\n",
    "    # mediante los métodos de reducción de dimensiones.\n",
    "    # TODO: Your code here!\n",
    "\n",
    "    # Calcule la precisión de knn en el conjunto de prueba transformado\n",
    "    # mediante los métodos de reducción de dimensiones.\n",
    "    acc_knn = 0 # TODO: Your code here!\n",
    "\n",
    "    # Representa el conjunto de datos en 2 dimensiones usando el modelo ajustado\n",
    "    X_embedded = model.transform(X)\n",
    "\n",
    "    # Grafica los puntos proyectados y mostrar el resultado de la evaluación\n",
    "    plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y, s=30, cmap=\"Set1\")\n",
    "    plt.title(\n",
    "        \"{}, KNN (k={})\\nTest accuracy = {:.2f}\".format(name, n_neighbors, acc_knn)\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d1cf80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "adf43341b7e3555f825c977766c277f7c0c126768e5bcd900173dd3621302e3d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
