{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP6  Aprendizaje de Máquinas\n",
    "---\n",
    "## Algoritmos de Clustering: K-Means y DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta clase práctica, exploramos cómo funcionan *K-means* y *DBSCAN*. Además, mostramos varios problemas de *K-means* y brindamos soluciones para abordar esos problemas, incluido el análisis de calidad de clúster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbpresent": {
     "id": "6f4fb01e-c707-4723-8cd2-4008f3ea3584"
    }
   },
   "outputs": [],
   "source": [
    "# Start from importing necessary packages.\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn import metrics # for evaluations\n",
    "from sklearn.datasets import make_blobs, make_circles # for generating experimental data\n",
    "from sklearn.preprocessing import StandardScaler # for feature scaling\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# make matplotlib plot inline (Only in Ipython).\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1: Análisis del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbpresent": {
     "id": "f0e44f6c-b6bf-4977-a44b-bf18c3e801e6"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generando datos.\n",
    "# `random_state` es la semilla utilizada por el generador de números aleatorios (random number generator) para la reproducibilidad (default=None).\n",
    "X, y = make_blobs(n_samples=5000,\n",
    "                  n_features=2,\n",
    "                  centers=3,\n",
    "                  random_state=170)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos nuestra matriz de características y el vector de clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbpresent": {
     "id": "16b9a559-9294-4cac-8921-7ac0bc46e34f"
    }
   },
   "outputs": [],
   "source": [
    "# Grafica la distribución de datos (_ground truth_) usando matplotlib `scatter(axis-x, axis-y, color)`.\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b292081a-becc-4c76-b7db-0643bcc3a7db"
    }
   },
   "source": [
    "### Ejercicio 2: Usando K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### El Algoritmo\n",
    "\n",
    "El algoritmo de agrupamiento de [Κ-means](https://scikit-learn.org/stable/modules/clustering.html#k-means) utiliza un refinamiento iterativo para producir un resultado final. Las entradas del algoritmo son el número de grupos $Κ$ y el conjunto de datos. El conjunto de datos es una colección de características para cada punto de datos. Los algoritmos comienzan con estimaciones iniciales para los centroides $Κ$, que pueden generarse aleatoriamente o seleccionarse aleatoriamente del conjunto de datos. Luego, el algoritmo itera entre dos pasos:\n",
    "\n",
    "**Paso de asignación de datos**: Cada centroide define uno de los clústeres. En este paso, cada punto de datos se asigna a su centroide más cercano, en función de la distancia euclidiana al cuadrado. Más formalmente, si $c_i$ es la colección de centroides en el conjunto $C$, entonces cada punto de datos $x$ se asigna a un grupo basado en\n",
    "\n",
    "$$\\underset{c_i \\in C}{\\arg\\min} \\; dist(c_i,x)^2$$\n",
    "donde dist( · ) es la distancia euclidiana estándar ($L_2$). Sea $S_i$ el conjunto de asignaciones de puntos de datos para cada i-ésimo centroide de los clusters.\n",
    "\n",
    "**Paso de actualización del centroide**: en este paso, se vuelven a calcular los centroides. Esto se hace tomando la media de todos los puntos de datos asignados al grupo de ese centroide.\n",
    "\n",
    "$$c_i=\\frac{1}{|S_i|}\\sum_{x_i \\in S_i }x_i$$\n",
    "\n",
    "El algoritmo itera entre los pasos uno y dos hasta que se cumple un criterio de parada (es decir, ningún punto de datos cambia de grupo, la suma de las distancias se minimiza, se alcanza un número máximo de iteraciones o el desplazamiento de los centroides es menor que un umbral determinado).\n",
    "\n",
    "**Convergencia e inicialización aleatoria**\n",
    "\n",
    "Este algoritmo está garantizado para converger a un resultado. El resultado puede ser un óptimo local (es decir, no necesariamente el mejor resultado posible), lo que significa que evaluar más de una ejecución del algoritmo con centroides iniciales aleatorios puede dar un mejor resultado.\n",
    "\n",
    "<img src=resources/K-means_convergence.gif style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilicemos K-means con nuestros datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbpresent": {
     "id": "db1a84f5-233c-45c1-b15e-989548255461"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" K-means clustering algorithm.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "n_init: int, optional, default: 10\n",
    "        Number of time the k-means algorithm will be run with different\n",
    "        centroid seeds. The final results will be the best output of\n",
    "        n_init consecutive runs in terms of inertia.\n",
    "        \n",
    "init: {'k-means++', 'random', or ndarray, or a callable}, optional\n",
    "        Method for initialization, default to 'k-means++'.\n",
    "        \n",
    "        'k-means++': selects initial cluster centers for k-mean\n",
    "        clustering in a smart way to speed up convergence.\n",
    "        \n",
    "        'random': generate k centroids from a Gaussian with mean and\n",
    "        variance estimated from the data.\n",
    "\n",
    "tol: float, default: 1e-4\n",
    "        Relative tolerance with regards to inertia to declare convergence\n",
    "        tolerance is computed using `np.mean(np.var(X, axis=0)) * tol)`\n",
    "\n",
    "\"\"\"\n",
    "kmeans = KMeans(n_clusters=3,\n",
    "                n_init=3,\n",
    "                init='random',\n",
    "                tol=1e-4, \n",
    "                random_state=170,\n",
    "                verbose=True).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mostrar las predicciones se usa la función `labels_` que devuelve un vector con los clusters a los cuales pertenece cada instancia de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbpresent": {
     "id": "676709d6-de25-41ee-a3f0-dfdb5be5b617"
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar predicciones\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para encontrar los centroides se usa la función `cluster_centers_` que devuelve una matriz con los centroides de cada grupo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar centros de clusters (centroides)\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafiquemos las predicciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbpresent": {
     "id": "f2d84569-10f0-4aca-a6e0-96624785bd88"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=kmeans.labels_)\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], \n",
    "            kmeans.cluster_centers_[:,1], \n",
    "            c='w', marker='x', linewidths=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos hacer nuevas predicciones sin volver a ejecutar kmeans (simplemente encuentra los centroides más cercanos ya computados para un nuevo conjunto de datos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuevo conjunto de datos\n",
    "X_new = np.array([[10,10], [-10, -10], [-5, 10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbpresent": {
     "id": "3a82e1e8-0875-4bac-b399-1f5a468c5306"
    }
   },
   "outputs": [],
   "source": [
    "# El código siguiente es equivalente a:\n",
    "# y_pred = KMeans(...).fit_predict(X), pero esto necesita realizar `fit` nuevamente.\n",
    "\n",
    "y_pred = kmeans.predict(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos obtener distancias desde el punto de datos hasta cada centroide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbpresent": {
     "id": "2be162d7-364f-4e7d-be4a-86018e786a5d"
    }
   },
   "outputs": [],
   "source": [
    "# El siguiente código es equivalente a:\n",
    "# from sklearn.metrics.pairwise import euclidean_distances\n",
    "# euclidean_distances(X_new, kmeans.cluster_centers_)\n",
    "\n",
    "kmeans.transform(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "84bf8a8a-edda-41a6-9a14-111a1c6e276e"
    }
   },
   "source": [
    "### Ejercicio 3: Problemas de K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1f05b8f1-015b-4ebd-9376-45464e73405e"
    }
   },
   "source": [
    "#### Problema 1: Necesita elegir un número correcto de clústeres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbpresent": {
     "id": "21d419d7-7866-4a78-8d01-f02bb60b1370"
    }
   },
   "outputs": [],
   "source": [
    "# Generar los datos.\n",
    "X, y = make_blobs(n_samples=1000,\n",
    "                  n_features=2,\n",
    "                  centers=3,\n",
    "                  random_state=170)\n",
    "\n",
    "# Graficar la distribución.\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbpresent": {
     "id": "fc2f3032-81c2-4c1b-8be0-55f4fd8fae68"
    }
   },
   "outputs": [],
   "source": [
    "# Ejecute k-means en datos no esféricos.\n",
    "y_pred = KMeans(n_clusters=2, random_state=170).fit_predict(X)\n",
    "\n",
    "# Graficar las predicciones.\n",
    "plt.scatter(X[:,0], X[:,1], c=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbpresent": {
     "id": "3a4feabc-8d61-4b8a-9fea-6f877c8d0b71"
    }
   },
   "outputs": [],
   "source": [
    "# Generar los datos.\n",
    "# Esta configuración particular tiene un grupo distinto y 3 grupos colocados muy juntos.\n",
    "X, y = make_blobs(n_samples=500,\n",
    "                  n_features=2,\n",
    "                  centers=4,\n",
    "                  cluster_std=1,\n",
    "                  center_box=(-10.0, 10.0),\n",
    "                  shuffle=True,\n",
    "                  random_state=1)\n",
    "\n",
    "# Graficar la distribución de los datos.\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ef047f7d-4411-43ef-83a2-f1f3c688981e"
    }
   },
   "source": [
    "#### Solución: medir la calidad de los clusters para determinar el número de clusters óptimo\n",
    "\n",
    "\n",
    "##### Método supervisado\n",
    "*Homogeneidad*: cada grupo contiene solo miembros de una sola clase.\n",
    "\n",
    "*Integridad*: Todos los miembros de una clase determinada se asignan al mismo grupo.\n",
    "\n",
    "##### Método no supervisado\n",
    "\n",
    "**Coeficiente de Sihouette** (*Sihouette Coefficient*): Evalua qué tan bien están la **compacidad** y la **separación** de los clusters.\n",
    "(Tenga en cuenta que la notación a continuación es consistente con el contenido anterior). Usando *Coeficiente de Sihouette*, podemos elegir un valor óptimo para el número de grupos.\n",
    "\n",
    "***\n",
    "\n",
    "$ a(x_i) $ denota la **distancia media dentro del grupo**. Evalua la compacidad del clúster al que pertenece $x_i$. (Cuanto más pequeño, más compacto)\n",
    "\n",
    "$$ a(x_i) = \\frac{ \\sum_{x_k \\in C_j ,\\ k \\neq i}{D(x_i, x_k)} }{\\left\\vert C_j \\right\\vert - 1} $$\n",
    "\n",
    "Para el punto de datos $x_i$, calcule su distancia promedio a todos los demás puntos de datos en su grupo. (Menos uno en la parte del denominador es omitir el punto de datos actual $x_i$)\n",
    "\n",
    "***\n",
    "\n",
    "$ b(x_i) $ denota la **distancia media del cluster más cercano**. Evalua cómo se separa $x_i$ de otros clústeres. (Cuanto más grande más separados)\n",
    "\n",
    "$$ b(x_i) = \\min_{C_j :\\ 1 \\leq j \\leq k ,\\ x_i \\notin C_j} \\left\\{ \\frac{ \\sum_{x_k \\in C_j}{D(x_i, x_k)} }{\\left\\vert C_j \\right\\vert } \\right\\} $$\n",
    "\n",
    "Para el punto de datos $x_i$ y todos los demás grupos que no contienen $x_i$, calcule su distancia promedio a todos los demás puntos de datos en los grupos dados. Encuentre el valor de distancia mínima con respecto a los grupos dados.\n",
    "\n",
    "***\n",
    "\n",
    "Finalmente, *Coeficiente de Sihouette*:  $s(x_i) = \\displaystyle\\frac{b(x_i) - a(x_i)}{\\max\\{a(x_i), b(x_i)\\}},\\ -1 \\leq s(x_i) \\leq 1 $. Se quiere $a(x_i) \\lt b(x_i)$ y $a(x_i) \\to 0$ así como $s(x_i) \\to 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, determinemos la cantidad de clusters óptimos basado en la homogeneidad, la integridad de los clusters y el Coeficiente de Sihouette. Para ello, se usan las funciones `metrics.homogeneity_score(y, y_pred)` y `metrics.completeness_score(y, y_pred)` para calcular los coeficientes de homogeneidad e integridad. La función `metrics.silhouette_samples(X, y_pred)` calcula el Coeficiente de Sihouette para cada punto de datos y `metrics.silhouette_score(X, y_pred)` calcula el Coeficiente de Sihouette medio de todos los puntos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbpresent": {
     "id": "b7dfa8af-03a6-4808-88b8-819d6500cbdd"
    }
   },
   "outputs": [],
   "source": [
    "# Lista de número de clústeres\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "# Para cada número de clusters, realice un análisis de Silhouette y visualice los resultados.\n",
    "for n_clusters in range_n_clusters:\n",
    "    \n",
    "    # Realizar k-means.\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    y_pred = kmeans.fit_predict(X)\n",
    "    \n",
    "    # Calcular la homogeneidad y la integridad de los clusters.\n",
    "    homogeneity = None\n",
    "    completeness = None\n",
    "    \n",
    "    # Calcular el coeficiente de coeficiente de Silhouette para cada muestra.\n",
    "    s = metrics.silhouette_samples(X, y_pred)\n",
    "    \n",
    "    # Calcule el coeficiente de Silhouette medio de todos los puntos de datos.\n",
    "    s_mean = metrics.silhouette_score(X, y_pred)\n",
    "    \n",
    "    # Para la configuración de los graficos -----------------------------------------------------------------------------------\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "    \n",
    "    # Configura el gráfico.\n",
    "    plt.suptitle('Silhouette analysis for K-Means clustering with n_clusters: {}'.format(n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Configura el 1er subgrafico.\n",
    "    ax1.set_title('Silhouette Coefficient for each sample')\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "    ax1.set_xlim([-1, 1])\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "    \n",
    "    # Configura el 2do subgrafico.\n",
    "    ax2.set_title('Homogeneity: {}, Completeness: {}, Mean Silhouette score: {}'.format(homogeneity,\n",
    "                                                                                        completeness,\n",
    "                                                                                        s_mean))\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "    \n",
    "    # Para el 1er subgráfico ------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Grafica el coeficiente de Silhouette para cada muestra.\n",
    "    cmap = cm.get_cmap(\"Spectral\")\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        ith_s = s[y_pred == i]\n",
    "        ith_s.sort()\n",
    "        size_cluster_i = ith_s.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        color = cmap(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_s,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10\n",
    "        \n",
    "    # Trazar el coeficiente de silueta medio utilizando la línea discontinua vertical roja.\n",
    "    ax1.axvline(x=s_mean, color=\"red\", linestyle=\"--\")\n",
    "    \n",
    "    # Para el 2do subgráfico ------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Grafica las predicciones\n",
    "    colors = cmap(y_pred.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:,0], X[:,1], c=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ab8a33f3-4dee-46e8-9abd-7d2fc56e63d8"
    }
   },
   "source": [
    "El gráfico de silueta muestra que el valor `n_clusters` de 3, 5 y 6 es una mala elección para los datos dados debido a la presencia de clústeres con puntajes de silueta por encima del promedio y también debido a las amplias fluctuaciones en el tamaño de los gráficos de silueta. El análisis de la silueta es más ambivalente al decidir entre 2 y 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ec546f74-36ca-4785-9ff5-1091a032b66c"
    }
   },
   "source": [
    "### Problema 2: No se pueden manejar datos con ruido y valores atípicos\n",
    "\n",
    "Incluso los datos de ruido y los valores atípicos se observan fácilmente a partir de los siguientes resultados de agrupación (los puntos de datos que están relativamente lejos de los centroides), *K-means* aún los coloca en las agrupaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbpresent": {
     "id": "37969ee2-e0ac-4dd9-8a02-13b393d5328b"
    }
   },
   "outputs": [],
   "source": [
    "# Generar datos.\n",
    "# Esta configuración en particular tiene un grupo distinto y 3 grupos colocados muy juntos.\n",
    "# (Igual que el ejemplo anterior)\n",
    "X, y = make_blobs(n_samples=500,\n",
    "                  n_features=2,\n",
    "                  centers=4,\n",
    "                  cluster_std=1,\n",
    "                  center_box=(-10.0, 10.0),\n",
    "                  shuffle=True,\n",
    "                  random_state=1)\n",
    "\n",
    "# Realiza k-means con n_clusters=4\n",
    "kmeans = KMeans(n_clusters=4, random_state=10)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "\n",
    "# Grafica la predicción\n",
    "plt.scatter(X[:,0], X[:,1], c=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a9eb7a4c-3079-4ad5-8d2e-9ff9d510c0b6"
    }
   },
   "source": [
    "##### Solución: Usar el umbral de distancia para detectar datos de ruido y valores atípicos\n",
    "\n",
    "Sin embargo, podemos detectar los ruidos/valores atípicos que condicionan si la distancia entre el punto de datos $x_i$ y el centroide $c_j$ del grupo correspondiente de $x_i$ es mayor que la distancia promedio en el grupo. Es decir:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  x_i=\\left\\{\n",
    "  \\begin{array}{ll}\n",
    "    \\text{Outlier}, & \\text{if}\\ D(x_i, c_j) \\gt \\frac{1}{\\left\\vert Cluster_j \\right\\vert} \\sum_{k=0,\\ k \\neq i}^{\\left\\vert Cluster_j \\right\\vert}{D(x_k,c_j)} \\\\\n",
    "    \\text{Non-outlier}, & \\text{otherwise}\n",
    "  \\end{array}\\right.\n",
    "  \\text{where } c_j \\in Cluster_j\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Comencemos a descubrir los valores atípicos de cada grupo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbpresent": {
     "id": "20ed43b9-96c5-4fd7-964b-a0a760556de3"
    }
   },
   "outputs": [],
   "source": [
    "# Relación para nuestro umbral de distancia, controlando cuántos valores atípicos queremos detectar.\n",
    "distance_threshold_ratio = 2.0\n",
    "\n",
    "# Grafica la predición como se hace más arriba\n",
    "plt.scatter(X[:,0], X[:,1], c=y_pred)\n",
    "\n",
    "# Para cada grupo i-ésimo, i=0~3 (tenemos 4 grupos en este ejemplo).\n",
    "for i in [0, 1, 2, 3]:\n",
    "    \n",
    "    # Recupere el índice de los puntos de datos que pertenecen al grupo i. \n",
    "    # Nota: `np.where()` devuelve los índices en una tupla, por lo que recuperamos los índices usando `tuple[0]`\n",
    "    indexs_of_X_in_ith_cluster = np.where(y_pred == i)[0]\n",
    "\n",
    "    # Recuperar los puntos de datos por los índices\n",
    "    X_in_ith_cluster = X[indexs_of_X_in_ith_cluster]\n",
    "    \n",
    "    # Recuperar el centroide.\n",
    "    centroid = kmeans.cluster_centers_[i].reshape(1, -1)\n",
    "\n",
    "    # Calcule las distancias entre los puntos de datos y el centroide.\n",
    "    # metrics.pairwise.euclidean_distances(X, Y) devuelve una matriz de distancias.\n",
    "    # Igual que: np.sqrt(np.sum(np.square(X - Y), axis=1))\n",
    "    # Nota: distances.shape = (X_in_ith_cluster.shape[0], 1). A 2-D matrix.\n",
    "    distances = metrics.pairwise.euclidean_distances(X_in_ith_cluster, centroid)\n",
    "    \n",
    "    # Calcule la distancia media para el grupo i-ésimo como nuestro umbral de distancia.\n",
    "    distance_threshold = np.mean(distances)\n",
    "    \n",
    "    # Recuperar el índice de valores atípicos en el grupo i-ésimo \n",
    "    # Nota: distances.flatten() aplana la matriz 2-D al vector, para comparar con `distance_threshold` escalar.\n",
    "    indexs_of_outlier = np.where(distances.flatten() > distance_threshold * distance_threshold_ratio)[0]\n",
    "    \n",
    "    # Recuperar valores atípicos en el clúster por los índices\n",
    "    outliers = X_in_ith_cluster[indexs_of_outlier]\n",
    "    \n",
    "    # Trazar los valores atípicos en i-ésimo grupo.\n",
    "    plt.scatter(outliers[:,0], outliers[:,1], c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a2bbd483-bb55-48f0-bc5a-16dc4d5a4fa3"
    }
   },
   "source": [
    "Como mencionamos sobre la medición del análisis de la calidad del clúster, puede ejecutar diferentes configuraciones de `distance_threshold_ratio` para encontrar la mejor calidad del clúster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "478a0112-e819-41ad-84dd-3f4be5bccf60"
    }
   },
   "source": [
    "#### Problema 3: No se pueden manejar datos no esféricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *K-means* tiene como objetivo dividir n observaciones en k grupos en los que cada observación pertenece al grupo con **la media más cercana**. (Wikipedia) \n",
    "\n",
    "Dado que los círculos concéntricos tendrían aproximadamente la misma media, k-means no es adecuado para separarlos.\n",
    "\n",
    "Generemos datos no esféricos y grafiquemos la distribución de los clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbpresent": {
     "id": "b06a8291-663b-4d92-89ca-033db1e0d247"
    }
   },
   "outputs": [],
   "source": [
    "# Generar datos no esféricos.\n",
    "X, y = make_circles(n_samples=1000, factor=0.3, noise=0.1)\n",
    "\n",
    "# Trazar la distribución de datos. (Aquí hay otra forma de trazar un gráfico de dispersión)\n",
    "plt.plot(X[y == 0, 0], X[y == 0, 1], 'ro')\n",
    "plt.plot(X[y == 1, 0], X[y == 1, 1], 'go')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de realizar *K-means* en datos no esféricos, el siguiente resultado muestra que no logra agrupar datos no esféricos, ya que *K-means* asume que la distribución de datos es esférica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbpresent": {
     "id": "c6d61883-e8b2-4a96-b93d-17f9e4e980b6"
    }
   },
   "outputs": [],
   "source": [
    "# Ejecutar k-means en datos no esféricos.\n",
    "y_pred = KMeans(n_clusters=2, random_state=170).fit_predict(X)\n",
    "\n",
    "# Graficar las predicciones.\n",
    "plt.plot(X[y_pred == 0, 0], X[y_pred == 0, 1], 'ro')\n",
    "plt.plot(X[y_pred == 1, 0], X[y_pred == 1, 1], 'go')\n",
    "\n",
    "# Imprimir las evaluaciones\n",
    "print('Homogeneity: {}'.format(metrics.homogeneity_score(y, y_pred)))\n",
    "print('Completeness: {}'.format(metrics.completeness_score(y, y_pred)))\n",
    "print('Mean Silhouette score: {}'.format(metrics.silhouette_score(X, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5572f1cd-9361-4ffb-aac4-b75850a6c6bd"
    }
   },
   "source": [
    "#### Solución: el uso de técnicas de extracción o transformación de características hace que los datos se puedan agrupar\n",
    "\n",
    "Si sabe que sus grupos siempre serán círculos concéntricos, simplemente puede convertir sus coordenadas cartesianas (x-y) en coordenadas polares y usar solo el radio para agrupar, ya que sabe que el ángulo theta no importa.\n",
    "\n",
    "O, de manera más general: use un kernel adecuado para la agrupación en clústeres de k-means, por ejemplo. use *Kernel PCA* para encontrar una proyección de los datos que los haga linealmente separables, o use otro algoritmo de agrupamiento, como *DBSCAN*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modo de ejemplo, convertamos las coordenadas cartesianas a polares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def cart2pol(x, y):\n",
    "    'Convierte coordenadas cartesianas (x-y) a polares.'\n",
    "    radius = np.sqrt(x**2 + y**2)\n",
    "    theta = np.arctan2(y, x)\n",
    "    return radius, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ilustremos cómo se verían los datos convertidos a polares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_transformed = np.zeros_like(X)\n",
    "X_transformed[:,0], X_transformed[:,1] = cart2pol(X[:,0], X[:,1])\n",
    "\n",
    "plt.plot(X_transformed[y == 0, 0], X_transformed[y == 0, 1], 'ro')\n",
    "plt.plot(X_transformed[y == 1, 0], X_transformed[y == 1, 1], 'go')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Simplemente logramos que los datos sean linealmente separables al convertir características (x-y) a (radio-theta)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, probemos `X_transformed` con *K-means*. Para esto, solo se usará la característica de radio (por lo que la característica *theta* se igualará a 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Solo use la característica `radius` para agrupar.\n",
    "X_transformed[:,1] = np.zeros_like(X_transformed[:,1]) \n",
    "y_pred = KMeans(n_clusters=2).fit_predict(X_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, grafiquemos la distribución de los datos de la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[y_pred == 0, 0], X[y_pred == 0, 1], 'ro')\n",
    "plt.plot(X[y_pred == 1, 0], X[y_pred == 1, 1], 'go')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9adbb49f-133f-497d-952c-256a473da64e"
    }
   },
   "source": [
    "### Ejercicio 4: Aplicando DBSCAN (_Density-Based Spatial Clustering of Applications with Noise_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo [DBSCAN](http://scikit-learn.org/stable/modules/clustering.html#dbscan) considera los clústeres como áreas de alta densidad separadas por áreas de baja densidad.\n",
    "\n",
    "Debido a esta vista bastante genérica, los grupos encontrados por DBSCAN pueden tener cualquier forma, a diferencia de k-means, que asume que los grupos tienen forma convexa.\n",
    "\n",
    "El componente central del DBSCAN es el **concepto de muestras de núcleo, que son muestras que se encuentran en áreas de alta densidad**. Por lo tanto, un cluster es un conjunto de muestras centrales, cada una cerca de la otra (medida por alguna medida de distancia) y un conjunto de muestras no centrales que están cerca de una muestra central (pero que no son muestras centrales en sí mismas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Parámetros\n",
    "- $Eps$: Radio máximo de la vecindad.\n",
    "- $MinPts$: Número mínimo de puntos en el Eps-vecindario de un punto.\n",
    "\n",
    "#### Términos\n",
    "- La vecindad Eps de un punto $q$－$N_{Eps}$: Un punto $p \\in N_{Eps}(q)$ si $D(p,q) \\leq Eps$. (Señale dentro del círculo).\n",
    "- Valor atípico: no en un clúster.\n",
    "- Punto central: $\\left\\vert N_{Eps}(q) \\right\\vert \\geq MinPts$ (vecindario denso).\n",
    "- Punto fronterizo: En clúster pero la vecindad no es densa.\n",
    "\n",
    "<div style=\"text-align:center\"><img width=\"300px\" src=\"resources/core-border-point.png\"/></div>\n",
    "\n",
    "\n",
    "- Directamente alcanzable por densidad: Un punto $p$ es **directamente alcanzable por densidad** desde un punto $q$ con $Eps$ y $MinPts$ si:\n",
    "    - $p \\in N_{Eps}(q)$, y $q$ es un **punto central**.\n",
    "    - $p$ **no** tiene que ser un punto central.\n",
    "\n",
    "<div style=\"text-align:center\"><img width=\"250px\" src=\"resources/directly-density-reachable.png\"/></div>\n",
    "\n",
    "- Densidad alcanzable: Un punto $p$ es **densidad alcanzable** desde un punto $q$ con respecto a $Eps$ y $MinPts$ si hay una cadena de puntos $p_1, \\dots, p_n,\\ p_1 = q,\\ p_n = p$ tal que $p_{i+1}$ es directamente accesible por densidad desde $p_i ps\n",
    "\n",
    "<div style=\"text-align:center\"><img width=\"150px\" src=\"resources/density-reachable.png\"/></div>\n",
    "\n",
    "#### El algoritmo\n",
    "1. Elige aleatoriamente un punto $p$.\n",
    "2. Recuperar todos los puntos de densidad alcanzable desde $p$ w.r.t. $Eps$ y $MinPts$.\n",
    "3. Si $p$ es un punto central, se forma un grupo.\n",
    "4. Si $p$ es un punto fronterizo, no se puede alcanzar la densidad de ningún punto desde $p$, luego visite el siguiente punto.\n",
    "5. Repita el proceso hasta que se hayan procesado todos los puntos de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comencemos a realizar *DBSCAN* en datos esféricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbpresent": {
     "id": "f3676519-cac4-4445-b29a-fb61e9ac8882"
    }
   },
   "outputs": [],
   "source": [
    "# Generar los datos con 3 centros.\n",
    "X, y = make_blobs(n_samples=1000, \n",
    "                  n_features=2, \n",
    "                  centers=3,\n",
    "                  random_state=170)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos `StandardScaler` para estandarizar características a media cero y varianza unitaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego realizamos DBSCAN en los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = DBSCAN(eps=0.3, min_samples=30).fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficamos las predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E imprimimos las evaluaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(X, y, y_pred):\n",
    "    print('Number of clusters: {}'.format(len(set(y_pred[np.where(y_pred != -1)]))))\n",
    "    print('Homogeneity: {}'.format(metrics.homogeneity_score(y, y_pred)))\n",
    "    print('Completeness: {}'.format(metrics.completeness_score(y, y_pred)))\n",
    "    print('Mean Silhouette score: {}'.format(metrics.silhouette_score(X, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(X, y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los puntos de datos **negros** indican los **valores atípicos** en el resultado anterior.\n",
    "\n",
    "Tenga en cuenta que no necesitamos especificar el número de clústeres con el algoritmo *DBSCAN*. Además, *DBSCAN* es bueno para descubrir los valores atípicos sin requerir algunos trucos como hicimos anteriormente en la sección *K-means*.\n",
    "\n",
    "Ahora, probemos *DBSCAN* con datos no esféricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbpresent": {
     "id": "4795d38a-ea38-49e7-9e3b-b9812c80f03b"
    }
   },
   "outputs": [],
   "source": [
    "# Generar los datos no esféricos\n",
    "X, y = make_circles(n_samples=1000, factor=0.3, noise=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, Usamos `StandardScaler` para estandarizar características a media cero y varianza unitaria, probamos DBSCAN en los datos y graficamos las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Estandariza las características a media cero y una unidad de varianza.\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Utilice DBSCAN en los datos.\n",
    "y_pred = DBSCAN(eps=0.3, min_samples=10).fit_predict(X)\n",
    "\n",
    "# Grafique la distribución de los datos\n",
    "plt.plot(X[y_pred == 0, 0], X[y_pred == 0, 1], 'ro')\n",
    "plt.plot(X[y_pred == 1, 0], X[y_pred == 1, 1], 'go')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, imprimimos las evaluaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(X, y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c95f7334-c3cb-467b-8bd0-f948a234a75e"
    }
   },
   "source": [
    "En comparación con *K-means*, podemos aplicar directamente *DBSCAN* en esta forma de distribución de datos debido al criterio de agrupamiento basado en la densidad.\n",
    "\n",
    "Nota: vale la pena mencionar que la *puntuación de Silhouette* es generalmente más alta para los clústeres **convexos** que para otros conceptos de clústeres, como los clústeres basados en la densidad."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "adf43341b7e3555f825c977766c277f7c0c126768e5bcd900173dd3621302e3d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
